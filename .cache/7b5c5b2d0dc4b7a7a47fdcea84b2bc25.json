{"dependencies":[{"name":"/home/szamulko/Desktop/Projects/PacMan/package.json","includedInParent":true,"mtime":1528724217926},{"name":"/home/szamulko/Desktop/Projects/PacMan/node_modules/@tensorflow/tfjs-core/package.json","includedInParent":true,"mtime":1528724212618},{"name":"../doc","loc":{"line":7,"column":20}},{"name":"./adadelta_optimizer","loc":{"line":8,"column":34}},{"name":"./adagrad_optimizer","loc":{"line":9,"column":33}},{"name":"./adam_optimizer","loc":{"line":10,"column":30}},{"name":"./adamax_optimizer","loc":{"line":11,"column":32}},{"name":"./momentum_optimizer","loc":{"line":12,"column":34}},{"name":"./rmsprop_optimizer","loc":{"line":13,"column":33}},{"name":"./sgd_optimizer","loc":{"line":14,"column":29}}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n    value: true\n});\nexports.OptimizerConstructors = undefined;\n\nvar _doc = require(\"../doc\");\n\nvar _adadelta_optimizer = require(\"./adadelta_optimizer\");\n\nvar _adagrad_optimizer = require(\"./adagrad_optimizer\");\n\nvar _adam_optimizer = require(\"./adam_optimizer\");\n\nvar _adamax_optimizer = require(\"./adamax_optimizer\");\n\nvar _momentum_optimizer = require(\"./momentum_optimizer\");\n\nvar _rmsprop_optimizer = require(\"./rmsprop_optimizer\");\n\nvar _sgd_optimizer = require(\"./sgd_optimizer\");\n\nvar __decorate = undefined && undefined.__decorate || function (decorators, target, key, desc) {\n    var c = arguments.length,\n        r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc,\n        d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\n\nvar OptimizerConstructors = function () {\n    function OptimizerConstructors() {}\n    OptimizerConstructors.sgd = function (learningRate) {\n        return new _sgd_optimizer.SGDOptimizer(learningRate);\n    };\n    OptimizerConstructors.momentum = function (learningRate, momentum, useNesterov) {\n        if (useNesterov === void 0) {\n            useNesterov = false;\n        }\n        return new _momentum_optimizer.MomentumOptimizer(learningRate, momentum, useNesterov);\n    };\n    OptimizerConstructors.rmsprop = function (learningRate, decay, momentum, epsilon, centered) {\n        if (decay === void 0) {\n            decay = .9;\n        }\n        if (momentum === void 0) {\n            momentum = 0.0;\n        }\n        if (epsilon === void 0) {\n            epsilon = 1e-8;\n        }\n        if (centered === void 0) {\n            centered = false;\n        }\n        return new _rmsprop_optimizer.RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n    };\n    OptimizerConstructors.adam = function (learningRate, beta1, beta2, epsilon) {\n        if (learningRate === void 0) {\n            learningRate = 0.001;\n        }\n        if (beta1 === void 0) {\n            beta1 = 0.9;\n        }\n        if (beta2 === void 0) {\n            beta2 = 0.999;\n        }\n        if (epsilon === void 0) {\n            epsilon = 1e-8;\n        }\n        return new _adam_optimizer.AdamOptimizer(learningRate, beta1, beta2, epsilon);\n    };\n    OptimizerConstructors.adadelta = function (learningRate, rho, epsilon) {\n        if (learningRate === void 0) {\n            learningRate = .001;\n        }\n        if (rho === void 0) {\n            rho = .95;\n        }\n        if (epsilon === void 0) {\n            epsilon = 1e-8;\n        }\n        return new _adadelta_optimizer.AdadeltaOptimizer(learningRate, rho, epsilon);\n    };\n    OptimizerConstructors.adamax = function (learningRate, beta1, beta2, epsilon, decay) {\n        if (learningRate === void 0) {\n            learningRate = 0.002;\n        }\n        if (beta1 === void 0) {\n            beta1 = 0.9;\n        }\n        if (beta2 === void 0) {\n            beta2 = 0.999;\n        }\n        if (epsilon === void 0) {\n            epsilon = 1e-8;\n        }\n        if (decay === void 0) {\n            decay = 0.0;\n        }\n        return new _adamax_optimizer.AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n    };\n    OptimizerConstructors.adagrad = function (learningRate, initialAccumulatorValue) {\n        if (initialAccumulatorValue === void 0) {\n            initialAccumulatorValue = 0.1;\n        }\n        return new _adagrad_optimizer.AdagradOptimizer(learningRate, initialAccumulatorValue);\n    };\n    __decorate([(0, _doc.doc)({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })], OptimizerConstructors, \"sgd\", null);\n    __decorate([(0, _doc.doc)({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })], OptimizerConstructors, \"momentum\", null);\n    __decorate([(0, _doc.doc)({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })], OptimizerConstructors, \"rmsprop\", null);\n    __decorate([(0, _doc.doc)({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })], OptimizerConstructors, \"adam\", null);\n    __decorate([(0, _doc.doc)({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })], OptimizerConstructors, \"adadelta\", null);\n    __decorate([(0, _doc.doc)({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })], OptimizerConstructors, \"adamax\", null);\n    __decorate([(0, _doc.doc)({ heading: 'Training', subheading: 'Optimizers', namespace: 'train' })], OptimizerConstructors, \"adagrad\", null);\n    return OptimizerConstructors;\n}();\nexports.OptimizerConstructors = OptimizerConstructors;\n//# sourceMappingURL=optimizer_constructors.js.map"},"hash":"42eb3bef680fe2a62e2e7372685afd38","cacheData":{"env":{}}}