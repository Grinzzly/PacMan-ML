{"dependencies":[{"name":"/home/szamulko/Desktop/Projects/PacMan/package.json","includedInParent":true,"mtime":1528724217926},{"name":"/home/szamulko/Desktop/Projects/PacMan/node_modules/@tensorflow/tfjs-core/package.json","includedInParent":true,"mtime":1528724212618},{"name":"../environment","loc":{"line":11,"column":20}},{"name":"../globals","loc":{"line":12,"column":27}},{"name":"../ops/ops","loc":{"line":13,"column":34}},{"name":"../serialization","loc":{"line":14,"column":33}},{"name":"./optimizer","loc":{"line":15,"column":26}}],"generated":{"js":"'use strict';\n\nObject.defineProperty(exports, \"__esModule\", {\n    value: true\n});\nexports.AdamaxOptimizer = undefined;\n\nvar _environment = require('../environment');\n\nvar _globals = require('../globals');\n\nvar _ops = require('../ops/ops');\n\nvar _serialization = require('../serialization');\n\nvar _optimizer = require('./optimizer');\n\nvar __extends = undefined && undefined.__extends || function () {\n    var extendStatics = Object.setPrototypeOf || { __proto__: [] } instanceof Array && function (d, b) {\n        d.__proto__ = b;\n    } || function (d, b) {\n        for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p];\n    };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() {\n            this.constructor = d;\n        }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n}();\n\nvar AdamaxOptimizer = function (_super) {\n    __extends(AdamaxOptimizer, _super);\n    function AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay) {\n        if (epsilon === void 0) {\n            epsilon = 1e-8;\n        }\n        if (decay === void 0) {\n            decay = 0.0;\n        }\n        var _this = _super.call(this) || this;\n        _this.learningRate = learningRate;\n        _this.beta1 = beta1;\n        _this.beta2 = beta2;\n        _this.epsilon = epsilon;\n        _this.decay = decay;\n        _this.accumulatedFirstMoment = {};\n        _this.accumulatedWeightedInfNorm = {};\n        _this.c = (0, _globals.keep)((0, _ops.scalar)(-learningRate));\n        _this.epsScalar = (0, _globals.keep)((0, _ops.scalar)(epsilon));\n        _this.beta1Scalar = (0, _globals.keep)((0, _ops.scalar)(beta1));\n        _this.beta2Scalar = (0, _globals.keep)((0, _ops.scalar)(beta2));\n        _this.decayScalar = (0, _globals.keep)((0, _ops.scalar)(decay));\n        (0, _globals.tidy)(function () {\n            _this.iteration = (0, _ops.scalar)(0).variable();\n            _this.accBeta1 = (0, _ops.scalar)(beta1).variable();\n        });\n        _this.oneMinusBeta1 = (0, _globals.keep)((0, _ops.scalar)(1 - beta1));\n        _this.one = (0, _globals.keep)((0, _ops.scalar)(1));\n        return _this;\n    }\n    AdamaxOptimizer.prototype.applyGradients = function (variableGradients) {\n        var _this = this;\n        (0, _globals.tidy)(function () {\n            var oneMinusAccBeta1 = _this.one.sub(_this.accBeta1);\n            var lr = _this.c.div(_this.one.add(_this.decayScalar.mul(_this.iteration)));\n            for (var variableName in variableGradients) {\n                var value = _environment.ENV.engine.registeredVariables[variableName];\n                if (_this.accumulatedFirstMoment[variableName] == null) {\n                    var trainable = false;\n                    _this.accumulatedFirstMoment[variableName] = (0, _ops.zerosLike)(value).variable(trainable);\n                }\n                if (_this.accumulatedWeightedInfNorm[variableName] == null) {\n                    var trainable = false;\n                    _this.accumulatedWeightedInfNorm[variableName] = (0, _ops.zerosLike)(value).variable(trainable);\n                }\n                var gradient = variableGradients[variableName];\n                var firstMoment = _this.accumulatedFirstMoment[variableName];\n                var weightedInfNorm = _this.accumulatedWeightedInfNorm[variableName];\n                var newFirstMoment = _this.beta1Scalar.mul(firstMoment).add(_this.oneMinusBeta1.mul(gradient));\n                var ut0 = _this.beta2Scalar.mul(weightedInfNorm);\n                var ut1 = gradient.abs();\n                var newWeightedInfNorm = ut0.maximum(ut1);\n                _this.accumulatedFirstMoment[variableName].assign(newFirstMoment);\n                _this.accumulatedWeightedInfNorm[variableName].assign(newWeightedInfNorm);\n                var newValue = lr.div(oneMinusAccBeta1).mul(newFirstMoment.div(_this.epsScalar.add(newWeightedInfNorm))).add(value);\n                value.assign(newValue);\n            }\n            _this.iteration.assign(_this.iteration.add(_this.one));\n            _this.accBeta1.assign(_this.accBeta1.mul(_this.beta1Scalar));\n        });\n    };\n    AdamaxOptimizer.prototype.dispose = function () {\n        var _this = this;\n        this.c.dispose();\n        this.epsScalar.dispose();\n        this.accBeta1.dispose();\n        this.beta1Scalar.dispose();\n        this.beta2Scalar.dispose();\n        this.oneMinusBeta1.dispose();\n        this.decayScalar.dispose();\n        this.iteration.dispose();\n        this.one.dispose();\n        if (this.accumulatedFirstMoment != null) {\n            Object.keys(this.accumulatedFirstMoment).forEach(function (name) {\n                return _this.accumulatedFirstMoment[name].dispose();\n            });\n        }\n        if (this.accumulatedWeightedInfNorm != null) {\n            Object.keys(this.accumulatedWeightedInfNorm).forEach(function (name) {\n                return _this.accumulatedWeightedInfNorm[name].dispose();\n            });\n        }\n    };\n    AdamaxOptimizer.prototype.getConfig = function () {\n        return {\n            learningRate: this.learningRate,\n            beta1: this.beta1,\n            beta2: this.beta2,\n            epsilon: this.epsilon,\n            decay: this.decay\n        };\n    };\n    AdamaxOptimizer.fromConfig = function (cls, config) {\n        return new cls(config.learningRate, config.beta1, config.beta2, config.epsilon, config.decay);\n    };\n    AdamaxOptimizer.className = 'AdamaxOptimizer';\n    return AdamaxOptimizer;\n}(_optimizer.Optimizer);\nexports.AdamaxOptimizer = AdamaxOptimizer;\n\n_serialization.SerializationMap.register(AdamaxOptimizer);\n//# sourceMappingURL=adamax_optimizer.js.map"},"hash":"7cb18c2ccdd3e3c8080e569a14a4cab3","cacheData":{"env":{}}}