{"dependencies":[{"name":"/home/szamulko/Desktop/Projects/PacMan/package.json","includedInParent":true,"mtime":1528724217926},{"name":"/home/szamulko/Desktop/Projects/PacMan/node_modules/@tensorflow/tfjs-core/package.json","includedInParent":true,"mtime":1528724212618},{"name":"../doc","loc":{"line":7,"column":20}},{"name":"../globals","loc":{"line":8,"column":27}},{"name":"../util","loc":{"line":9,"column":22}},{"name":"./axis_util","loc":{"line":10,"column":27}},{"name":"./operation","loc":{"line":11,"column":26}},{"name":"./ops","loc":{"line":12,"column":21}}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n    value: true\n});\nexports.SoftmaxOps = undefined;\n\nvar _doc = require(\"../doc\");\n\nvar _globals = require(\"../globals\");\n\nvar _util = require(\"../util\");\n\nvar util = _interopRequireWildcard(_util);\n\nvar _axis_util = require(\"./axis_util\");\n\nvar axis_util = _interopRequireWildcard(_axis_util);\n\nvar _operation = require(\"./operation\");\n\nvar _ops = require(\"./ops\");\n\nvar ops = _interopRequireWildcard(_ops);\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }\n\nvar __decorate = undefined && undefined.__decorate || function (decorators, target, key, desc) {\n    var c = arguments.length,\n        r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc,\n        d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\n\nvar SoftmaxOps = function () {\n    function SoftmaxOps() {}\n    SoftmaxOps.softmax = function (logits, dim) {\n        if (dim === void 0) {\n            dim = -1;\n        }\n        util.assertArgumentsAreTensors({ logits: logits }, 'softmax');\n        if (dim === -1) {\n            dim = logits.rank - 1;\n        }\n        if (dim !== logits.rank - 1) {\n            throw Error('Softmax along a non-last dimension is not yet supported. ' + (\"Logits was rank \" + logits.rank + \" and dim was \" + dim));\n        }\n        var customOp = (0, _globals.customGrad)(function (logits) {\n            var keepDims = true;\n            var lse = logits.logSumExp([dim], keepDims);\n            var logResult = logits.toFloat().sub(lse);\n            var y = logResult.exp();\n            var gradFunc = function (dy) {\n                var dyTimesY = dy.mul(y);\n                var keepDims = true;\n                return dyTimesY.sub(dyTimesY.sum([dim], keepDims).mul(y));\n            };\n            return { value: y, gradFunc: gradFunc };\n        });\n        return customOp(logits);\n    };\n    SoftmaxOps.softmaxCrossEntropy = function (labels, logits, dim) {\n        if (dim === void 0) {\n            dim = -1;\n        }\n        util.assertArgumentsAreTensors({ labels: labels, logits: logits }, 'softmaxCrossEntropy');\n        util.assertShapesMatch(labels.shape, logits.shape, 'Error in softmaxCrossEntropy: ');\n        if (dim === -1) {\n            dim = logits.rank - 1;\n        }\n        if (dim !== logits.rank - 1) {\n            throw Error(\"Softmax cross entropy along a non-last dimension is not yet \" + (\"supported. Labels / logits was rank \" + logits.rank + \" \") + (\"and dim was \" + dim));\n        }\n        var customOp = (0, _globals.customGrad)(function (labels, logits) {\n            var predictedProbs = logits.softmax(dim);\n            var costVector = ops.scalar(1e-5).add(predictedProbs).log().mul(labels).neg();\n            var value = costVector.sum([dim]);\n            var gradFunc = function (dy) {\n                var dyShape = axis_util.expandShapeToKeepDim(dy.shape, [dim]);\n                return [dy.reshape(dyShape).mul(labels.toFloat().sub(predictedProbs)), dy.reshape(dyShape).mul(predictedProbs.sub(labels.toFloat()))];\n            };\n            return { value: value, gradFunc: gradFunc };\n        });\n        return customOp(labels, logits);\n    };\n    __decorate([(0, _doc.doc)({ heading: 'Operations', subheading: 'Normalization' }), _operation.operation], SoftmaxOps, \"softmax\", null);\n    __decorate([(0, _doc.doc)({ heading: 'Training', subheading: 'Losses', namespace: 'losses' }), _operation.operation], SoftmaxOps, \"softmaxCrossEntropy\", null);\n    return SoftmaxOps;\n}();\nexports.SoftmaxOps = SoftmaxOps;\n//# sourceMappingURL=softmax.js.map"},"hash":"b0137ce3227a74893ec9c0471c6ae45c","cacheData":{"env":{}}}